Scientific AGI should be organized as a hierarchical system of epistemic agents,
where informational entropy increases downward while model complexity and cognitive
resources increase upward.

Top-level AGI scientists operate with the most powerful and expensive models (some
new cognitive architecture like Continuous Thought Machines with higher levels of
transparency), focusing on theory synthesis and paradigm-level error correction or
other high level task. Basically it has master plan of whole research.
Lower levels rely on transformer-based LLMs to perform large-scale exploration,
filtering, and preliminary evaluation of information.
This separation allows scientific intelligence to scale efficiently without exposing the
core reasoning system to raw informational noise. Basically this is core isolation idea
from Linux and a reliable system built from unreliable components

Lower-level transformer agents operate under strict goal insulation: they do not see the
top-level research agenda, do not handle high-stakes decisions, and produce only
auditable candidate signals. Their outputs are treated as hypotheses to be cross-
checked—via redundancy, critic models, and source verification—before any
information is allowed to influence the top-level scientific model. For example such
untransparent agent can solve complicated mathematical tasks like integrals

That is, in principle, it should be possible to learn how to use LLMs safely as
subsystems within an AGI architecture.
